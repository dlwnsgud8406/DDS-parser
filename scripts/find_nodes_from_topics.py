#!/usr/bin/env python3
"""
ROS2 ë…¸ë“œ ì´ë¦„ì„ í† í”½ì—ì„œ ì—­ì¶”ì 

ROS2ì—ì„œëŠ” ë…¸ë“œê°€ ë‹¤ìŒê³¼ ê°™ì€ í† í”½ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤:
- /rosout
- /parameter_events
- rq/<node_name>/...   (request)
- rr/<node_name>/...   (response)  
- rt/<node_name>/...   (regular topic)
"""

import sys
from pathlib import Path
from collections import defaultdict
import re

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.packet_source import PcapSource
from src.parser import EnhancedRTPSParser
from src.sink import DataFrameSink
from src.processor import TimeWindowProcessor
from src.transformer import EndpointMapper
from src.transformer.node_extractor import NodeNameExtractor


def main():
    pcap_file = "data/shm.pcapng"
    max_packets = 5000
    
    print("=" * 80)
    print("ROS2 ë…¸ë“œ ì´ë¦„ ì—­ì¶”ì  (í† í”½ ê¸°ë°˜)")
    print("=" * 80)
    print(f"íŒŒì¼: {pcap_file}")
    print(f"ìµœëŒ€ íŒ¨í‚·: {max_packets}")
    print()
    
    # ì˜ˆìƒë˜ëŠ” ë…¸ë“œ ì´ë¦„ë“¤ (ì •ê·œí™”ëœ ë²„ì „)
    expected_nodes = {
        'joint_state_publisher',
        'launch_ros_*',  # launch_ros_2385 ì •ê·œí™”
        'robot_state_publisher',
        'static_tf_pub_laser',
        'stella_ahrs_node',
        'stella_md_node',
        'ydlidar_ros2_driver_node',
    }
    
    print("ğŸ¯ ì°¾ì•„ì•¼ í•  ë…¸ë“œ ì´ë¦„:")
    for node in sorted(expected_nodes):
        print(f"   â€¢ /{node}")
    print()
    
    # 1. PCAP íŒŒì‹±
    print("[1/3] PCAP íŒŒì¼ íŒŒì‹± ì¤‘...")
    source = PcapSource(pcap_file)
    parser = EnhancedRTPSParser()
    sink = DataFrameSink()
    processor = TimeWindowProcessor(window_seconds=1.0, max_packets=max_packets)
    
    processor.process_stream(source, parser, sink)
    df = sink.get_result()
    submessages = df.to_dict('records')
    print(f"  âœ“ {len(submessages):,}ê°œ submessage íŒŒì‹± ì™„ë£Œ\n")
    
    # 2. Endpoint â†’ Topic ë§¤í•‘
    print("[2/3] SEDP ë§¤í•‘ í…Œì´ë¸” ìƒì„±...")
    endpoint_mapper = EndpointMapper()
    endpoint_mapper.build_mapping(submessages)
    
    stats = endpoint_mapper.get_statistics()
    print(f"  âœ“ {stats['total_endpoints']}ê°œ endpoint ë°œê²¬")
    print(f"  âœ“ {stats['topics_count']}ê°œ topic ë°œê²¬\n")
    
    # 3. í† í”½ì—ì„œ ë…¸ë“œ ì´ë¦„ ì¶”ì¶œ
    print("[3/3] í† í”½ ë¶„ì„ ë° ë…¸ë“œ ì¶”ì¶œ")
    print("=" * 80)
    
    all_topics = sorted(stats['topics'])
    
    # ë…¸ë“œ ì¶”ì¶œê¸° ìƒì„±
    extractor = NodeNameExtractor()
    
    # ë…¸ë“œë³„ í† í”½ ê·¸ë£¹í™”
    node_topics = defaultdict(set)
    unknown_topics = []
    
    for topic in all_topics:
        node_name = extractor.extract_node_from_topic(topic)
        if node_name:
            node_topics[node_name].add(topic)
        else:
            unknown_topics.append(topic)
    
    # ë°œê²¬ëœ ë…¸ë“œ ì¶œë ¥
    if node_topics:
        print(f"\nâœ… {len(node_topics)}ê°œì˜ ë…¸ë“œ ë°œê²¬!\n")
        
        for i, (node_name, topics) in enumerate(sorted(node_topics.items()), 1):
            # ì˜ˆìƒ ë…¸ë“œì¸ì§€ í™•ì¸
            is_expected = node_name in expected_nodes
            marker = "âœ…" if is_expected else "ğŸ”"
            
            print(f"{marker} {i}. /{node_name}")
            print(f"   í† í”½ ìˆ˜: {len(topics)}ê°œ")
            
            # í† í”½ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ)
            for topic in sorted(topics)[:5]:
                print(f"     â€¢ {topic}")
            if len(topics) > 5:
                print(f"     ... ì™¸ {len(topics) - 5}ê°œ")
            print()
        
        # ì˜ˆìƒ ë…¸ë“œì™€ ë¹„êµ
        found_nodes = set(node_topics.keys())
        missing = expected_nodes - found_nodes
        extra = found_nodes - expected_nodes
        
        print("=" * 80)
        print("ë…¸ë“œ ë§¤ì¹­ ê²°ê³¼:")
        print("=" * 80)
        print(f"ì˜ˆìƒ ë…¸ë“œ: {len(expected_nodes)}ê°œ")
        print(f"ë°œê²¬ ë…¸ë“œ: {len(found_nodes)}ê°œ")
        print(f"ì¼ì¹˜: {len(found_nodes & expected_nodes)}ê°œ")
        
        if missing:
            print(f"\nâŒ ì°¾ì§€ ëª»í•œ ë…¸ë“œ ({len(missing)}ê°œ):")
            for node in sorted(missing):
                print(f"   â€¢ /{node}")
        
        if extra:
            print(f"\nğŸ” ì˜ˆìƒ ì™¸ ë…¸ë“œ ({len(extra)}ê°œ):")
            for node in sorted(extra):
                print(f"   â€¢ /{node}")
    else:
        print("âŒ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
    
    # íŒ¨í„´ ì—†ëŠ” í† í”½ë“¤
    if unknown_topics:
        print(f"\nê¸°íƒ€ í† í”½ ({len(unknown_topics)}ê°œ):")
        for topic in unknown_topics[:10]:
            print(f"   â€¢ {topic}")
        if len(unknown_topics) > 10:
            print(f"   ... ì™¸ {len(unknown_topics) - 10}ê°œ")
    
    # Participant GUID â†’ ë…¸ë“œ ë§¤í•‘ ì¶”ì¶œ
    print("\n" + "=" * 80)
    print("Participant GUID â†’ ë…¸ë“œ ë§¤í•‘:")
    print("=" * 80)
    
    # endpoint_mapì—ì„œ ì—­ì¶”ì 
    guid_to_nodes = defaultdict(set)
    
    for endpoint_guid, info in endpoint_mapper.endpoint_map.items():
        topic = info.get('topic', '')
        node_name = extractor.extract_node_from_topic(topic)
        
        if node_name:
            # Participant GUID = (hostId, appId, instanceId)
            participant_guid = endpoint_guid[:3]  # entityId ì œì™¸
            guid_to_nodes[participant_guid].add(node_name)
    
    if guid_to_nodes:
        print()
        for i, (pguid, nodes) in enumerate(sorted(guid_to_nodes.items()), 1):
            hostId, appId, instanceId = pguid
            print(f"{i}. Participant ({hostId:08x}, {appId:08x}, {instanceId:08x})")
            for node in sorted(nodes):
                print(f"   â†’ /{node}")
            print()
    else:
        print("  (ë§¤í•‘ ì •ë³´ ì—†ìŒ)")


if __name__ == "__main__":
    main()
