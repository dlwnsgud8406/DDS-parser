#!/usr/bin/env python3
"""
PCAP â†’ Excel ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

PCAP íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ v2.2 í˜•ì‹ì˜ Excel íŒŒì¼ ìƒì„±
"""

import sys
import argparse
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.packet_source import PcapSource
from src.parser import EnhancedRTPSParser
from src.sink import DataFrameSink
from src.processor import TimeWindowProcessor
from src.transformer import TopicGrouper, NodeGrouper, EndpointMapper, PivotTableBuilder, QoSAnalyzer
from src.excel_writer import ExcelWriter
import pandas as pd
from collections import defaultdict, Counter


def main():
    parser = argparse.ArgumentParser(
        description="PCAP â†’ Excel ë³€í™˜ (v2.2 Multi-Row)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
    python scripts/generate_excel.py data/participated_57_first_bye.pcapng
    python scripts/generate_excel.py data/rtps_only_stream.pcapng -o output/stream.xlsx
    python scripts/generate_excel.py data/test.pcapng -w 2.0 -n 1000
        """
    )
    
    parser.add_argument(
        'pcap_file',
        help='ì…ë ¥ PCAP íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '-o', '--output',
        default=None,
        help='ì¶œë ¥ Excel íŒŒì¼ ê²½ë¡œ [ê¸°ë³¸ê°’: output/<pcap_name>_analysis.xlsx]'
    )
    
    parser.add_argument(
        '-w', '--window',
        type=float,
        default=1.0,
        help='ì‹œê°„ ìœˆë„ìš° í¬ê¸° (ì´ˆ) [ê¸°ë³¸ê°’: 1.0]'
    )
    
    parser.add_argument(
        '-n', '--max-packets',
        type=int,
        default=None,
        help='ìµœëŒ€ ì²˜ë¦¬ íŒ¨í‚· ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©) [ê¸°ë³¸ê°’: ì œí•œ ì—†ìŒ]'
    )
    
    args = parser.parse_args()
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if args.output is None:
        pcap_name = Path(args.pcap_file).stem
        output_path = f"output/{pcap_name}_analysis.xlsx"
    else:
        output_path = args.output
    
    print("=" * 80)
    print("RTPS PCAP â†’ Excel ë³€í™˜ (v2.2)")
    print("=" * 80)
    print(f"ì…ë ¥ íŒŒì¼: {args.pcap_file}")
    print(f"ì¶œë ¥ íŒŒì¼: {output_path}")
    print(f"ìœˆë„ìš° í¬ê¸°: {args.window}ì´ˆ")
    if args.max_packets:
        print(f"ìµœëŒ€ íŒ¨í‚· ìˆ˜: {args.max_packets:,}")
    print("=" * 80)
    
    try:
        # 1. PCAP íŒŒì‹±
        print("\n[1/5] PCAP íŒŒì¼ íŒŒì‹±...")
        source = PcapSource(args.pcap_file)
        parser = EnhancedRTPSParser()
        sink = DataFrameSink()
        processor = TimeWindowProcessor(
            window_seconds=args.window,
            max_packets=args.max_packets
        )
        
        processor.process_stream(source, parser, sink)
        df = sink.get_result()
        
        submessages = df.to_dict('records')
        print(f"  âœ“ {len(submessages):,}ê°œ submessage íŒŒì‹± ì™„ë£Œ")
        
        # 2. Endpoint â†’ Topic ë§¤í•‘ ìƒì„±
        print("\n[2/6] SEDP ë§¤í•‘ í…Œì´ë¸” ìƒì„±...")
        print(f"  â†’ {len(submessages):,}ê°œ submessage ë¶„ì„ ì¤‘...")
        endpoint_mapper = EndpointMapper()
        endpoint_mapper.build_mapping(submessages)
        
        stats = endpoint_mapper.get_statistics()
        print(f"  âœ“ {stats['total_endpoints']}ê°œ endpoint ë°œê²¬")
        print(f"    - Writers: {stats['writers']}")
        print(f"    - Readers: {stats['readers']}")
        print(f"    - Topics: {stats['topics_count']}")
        
        if stats['topics_count'] > 0:
            print(f"    ğŸ“‹ Topic ëª©ë¡:")
            for topic in sorted(stats['topics']):
                print(f"       â€¢ {topic}")
        
        # 3. ëª¨ë“  submessageì— topic ì •ë³´ ì¶”ê°€
        print("\n[3/6] Submessageì— Topic ë§¤í•‘...")
        print(f"  â†’ Endpoint ì •ë³´ë¡œ ì—­ë§¤í•‘ ì¤‘...")
        enriched_submessages = endpoint_mapper.enrich_submessages(submessages)
        
        mapped_count = sum(1 for msg in enriched_submessages if 'topic' in msg)
        unmapped_count = len(enriched_submessages) - mapped_count
        print(f"  âœ“ {mapped_count:,}/{len(enriched_submessages):,}ê°œ submessageì— topic ë§¤í•‘ë¨")
        if unmapped_count > 0:
            print(f"    âš ï¸  {unmapped_count:,}ê°œëŠ” topic ì •ë³´ ì—†ìŒ (SPDP, INFO ë“±)")
        
        # 4. ë©”ì‹œì§€ íƒ€ì…ë³„ ë¶„ë¦¬
        print("\n[4/6] ë©”ì‹œì§€ íƒ€ì…ë³„ ë¶„ë¦¬...")
        topic_grouper = TopicGrouper()
        separated = topic_grouper.separate_by_message_type(enriched_submessages)
        
        print(f"  âœ“ SPDP: {len(separated['spdp'])}ê°œ")
        print(f"  âœ“ SEDP Writers: {len(separated['sedp_writers'])}ê°œ")
        print(f"  âœ“ SEDP Readers: {len(separated['sedp_readers'])}ê°œ")
        print(f"  âœ“ User Traffic: {len(separated['user_traffic'])}ê°œ")
        
        # 5. QoS ì¶”ë¡ 
        print("\n[5/7] QoS ì •ì±… ì¶”ë¡ ...")
        qos_analyzer = QoSAnalyzer()
        topic_qos_map = qos_analyzer.analyze_messages(enriched_submessages)
        print(f"  âœ“ {len(topic_qos_map)}ê°œ í† í”½ì˜ QoS ì¶”ë¡  ì™„ë£Œ")
        
        # 6. ë…¸ë“œë³„ ê·¸ë£¹í™”
        print("\n[6/7] ROS2 ë…¸ë“œë³„ ê·¸ë£¹í™”...")
        node_grouper = NodeGrouper()
        grouped_by_node = node_grouper.group_by_node(enriched_submessages)
        print(f"  âœ“ {len(grouped_by_node)}ê°œ ë…¸ë“œ ë°œê²¬")
        
        if len(grouped_by_node) == 0:
            print("  âš ï¸  ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤. Excel ìƒì„±ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        # 7. Pivot Table ìƒì„± (ë…¸ë“œë³„) + QoS ì¶”ê°€
        print("\n[7/8] ë…¸ë“œë³„ Pivot Table ìƒì„± + QoS ë§¤í•‘...")
        builder = PivotTableBuilder(window_size=args.window)
        pivot_tables = {}
        
        for node_name, messages in grouped_by_node.items():
            df_pivot = builder.build(messages, participant_id=None)
            
            # QoS ì •ë³´ ì¶”ê°€
            if 'topic' in df_pivot.columns:
                df_pivot['reliability'] = df_pivot['topic'].map(
                    lambda t: topic_qos_map.get(t, {}).get('reliability', '')
                )
                df_pivot['durability'] = df_pivot['topic'].map(
                    lambda t: topic_qos_map.get(t, {}).get('durability', '')
                )
                df_pivot['frequency_hz'] = df_pivot['topic'].map(
                    lambda t: topic_qos_map.get(t, {}).get('frequency_hz', 0)
                )
            
            # ì‹œíŠ¸ëª… ìƒì„±
            sheet_name = node_grouper.format_node_name_for_sheet(node_name)
            
            pivot_tables[sheet_name] = df_pivot
            print(f"  âœ“ {sheet_name}: {len(df_pivot):,} í–‰ ({len(messages)} messages)")
        
        # Overview ë° QoS Summary ìƒì„±
        print("\n[8/9] Overview, QoS Summary, SEDP ì‹œíŠ¸ ìƒì„±...")
        
        # QoS Summary ë°ì´í„°
        qos_summary_data = qos_analyzer.get_qos_summary_dataframe()
        df_qos_summary = pd.DataFrame(qos_summary_data)
        print(f"  âœ“ QoS Summary ì‹œíŠ¸: {len(df_qos_summary)} í† í”½")
        
        node_summary = node_grouper.get_node_summary(grouped_by_node)
        overview_data = []
        
        for summary in node_summary:
            sheet_name = node_grouper.format_node_name_for_sheet(summary['node_name'])
            
            # Submessage typesë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            submsg_types_str = ", ".join(
                f"{k}({v})" for k, v in sorted(summary['submsg_types'].items())
            )
            
            overview_data.append({
                'Node': summary['node_name'],
                'Sheet Name': sheet_name,
                'Messages': summary['message_count'],
                'Topics': summary['topic_count'],
                'Types': submsg_types_str
            })
        
        df_overview = pd.DataFrame(overview_data)
        print(f"  âœ“ Overview ì‹œíŠ¸ ìƒì„±: {len(df_overview)} ë…¸ë“œ")
        
        # SEDP ì‹œíŠ¸ ë°ì´í„° ì¤€ë¹„
        sedp_data = endpoint_mapper.get_sedp_dataframe_data()
        df_sedp = pd.DataFrame(sedp_data)
        print(f"  âœ“ SEDP ì‹œíŠ¸: {len(df_sedp)} endpoints")
        
        # 9. Excel ì“°ê¸°
        print("\n[9/9] Excel ì“°ê¸°...")
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        writer = ExcelWriter(output_path)
        
        # Overview ì‹œíŠ¸
        writer.write_overview(df_overview)
        print(f"  âœ“ Overview ì‹œíŠ¸ ì‘ì„±")
        
        # QoS Summary ì‹œíŠ¸
        writer.write_qos_summary(df_qos_summary)
        print(f"  âœ“ QoS Summary ì‹œíŠ¸ ì‘ì„±")
        
        # SEDP ì‹œíŠ¸ (ê°„ë‹¨í•œ í…Œì´ë¸”)
        writer.write_sedp_sheet(df_sedp)
        print(f"  âœ“ SEDP ì‹œíŠ¸ ì‘ì„±")
        
        # ë…¸ë“œë³„ ì‹œíŠ¸
        writer.write_node_sheets(pivot_tables)
        print(f"  âœ“ {len(pivot_tables)}ê°œ ë…¸ë“œ ì‹œíŠ¸ ì‘ì„±")
        
        writer.save()
        print(f"  âœ“ íŒŒì¼ ì €ì¥: {output_path}")
        
        # 9. ìµœì¢… ìš”ì•½
        print("\n" + "=" * 80)
        print("âœ… ì™„ë£Œ!")
        print("=" * 80)
        
        file_size = Path(output_path).stat().st_size
        total_excel_rows = sum(len(df) for df in pivot_tables.values())
        
        print(f"íŒŒì¼: {output_path}")
        print(f"í¬ê¸°: {file_size / 1024:.1f} KB ({file_size / (1024*1024):.2f} MB)")
        print(f"\nì‹œíŠ¸ êµ¬ì„±:")
        print(f"  - Overview: {len(df_overview)} ë…¸ë“œ")
        print(f"  - SEDP: {len(df_sedp)} endpoints")
        print(f"  - ë…¸ë“œ ì‹œíŠ¸: {len(pivot_tables)}ê°œ")
        print(f"\në°ì´í„°:")
        print(f"  - ì´ Submessages: {len(submessages):,}ê°œ")
        print(f"  - Topic ë§¤í•‘ë¨: {mapped_count:,}ê°œ")
        print(f"  - Excel í–‰: {total_excel_rows:,}ê°œ")
        print(f"\në…¸ë“œ ëª©ë¡:")
        for i, node_name in enumerate(sorted(grouped_by_node.keys()), start=1):
            msg_count = len(grouped_by_node[node_name])
            print(f"  {i}. {node_name} ({msg_count:,} messages)")
        
    except FileNotFoundError as e:
        print(f"\nâŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    sys.exit(main())
